---
title: "E. None of the Above"
description: "An interactive blog post with multiple choice questions that explores how we can better prepare students for real-world decision making."
date: "2024-03-20"
author: "Matthew Young"
readTime: "5 min read"
type: "multiple-choice"
featured: true
tags: ["education", "interactive", "critical-thinking"]
---

# Interactive Setup

This post demonstrates how real-world decisions differ from multiple choice questions.

## Question
Your fast-growing startup just raised Series A funding. What should you do about hiring and remote work? Select the correct answer:

## Options
A. Hire globally and go fully remote to access the best talent worldwide at competitive rates
B. Keep everyone local and in-office to maintain culture, collaboration, and rapid iteration
C. Build hybrid 'hubs' in 3-4 major cities to balance talent access with in-person connection
D. Stay local for core team but hire remote specialists for specific skills you can't find nearby

## Follow-up
- **Question**: "So which is the correct answer?"
- **Subheading**: "Is there a correct answer?"

---

# This is what a real question looks like.

Yet, to prepare kids for this we simplify the world. Tell them we know all the answers. That's what school teaches you: the answers.

Life is not chess, it's poker. There's uncertainty. The "right choice," if there is one, doesn't guarantee the best outcome.

I get why we do this. Multiple choice is efficient. It scales. It's fair. We can teach thousands of kids with limited resources.

But we've created a trap. We simplify to evaluate at scale. Then we can only teach what fits those evaluations. The measurement tool shapes what we measure, and what we measure shapes what we teach.

Even when we try to teach critical thinking, we test it with bubble sheets. A kid who guessed right looks identical to one who reasoned brilliantly. We've optimized for the shadow of learning, not learning itself.

Teachers know this. They see how students actually think. But one teacher can only deeply evaluate so many students. The system forces them back to standardized tests.

Here's what's changing: the constraint was always human attention. One expert could only guide a handful of students. That's why personalized tutoring worked but couldn't scale.

What if that constraint is dissolving?

Imagine a teacher assigns the Remote Work dilemma. Students write their actual analysis. AI helps the teacher see every student's reasoning: who considered trade-offs, who wrestled with uncertainty, who reached for false certainty. Thirty students, each getting the attention that used to be possible for three.

Sure, AI evaluation could create new biases. It might reward certain thinking styles over others.

But here's the difference: those problems are fixable. Multiple choice, by its nature, can't evaluate complex thinking. AI potentially can, even if imperfectly at first.

The real question isn't whether AI can evaluate complex thinking. It's whether we're ready to admit that complex thinking is what we should have been teaching all along.

Most important questions don't have correct answers. Expertise isn't about knowing facts but navigating uncertainty.

The tools to break the cycle might finally be here. Are we brave enough to use them?